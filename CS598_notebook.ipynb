{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbwhaJnibOZ4",
        "outputId": "0820c33b-1483-4b86-f343-b42c05ee8d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import sys\n",
        "folder_path = '/content/drive/MyDrive/HiCu-ICD-main'\n",
        "sys.path.append('/content/drive/MyDrive/CS598_DLH_Project/HiCu-ICD-main')\n",
        "sys.path.append('/content/drive/MyDrive/CS598_DLH_Project/HiCu-ICD-main/utils')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/HiCu-ICD-main/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHNskVNNN86s",
        "outputId": "03bdb61a-a27b-498b-93b5-34086ee4c3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1i33jnEExQNdrh2x4f7zRXTC8io_INa2u/HiCu-ICD-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_ as xavier_uniform\n",
        "import numpy as np\n",
        "from math import floor, sqrt\n",
        "# creates word representations for a text classification task using CNN architecture.\n",
        "class WordRep(nn.Module):\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(WordRep, self).__init__()\n",
        "\n",
        "        if args.embed_file:\n",
        "            print(\"loading pretrained embeddings from {}\".format(args.embed_file))\n",
        "            if args.use_ext_emb:\n",
        "                pretrain_word_embedding, pretrain_emb_dim = build_pretrain_embedding(args.embed_file, dicts['w2ind'],\n",
        "                                                                                     True)\n",
        "                W = torch.from_numpy(pretrain_word_embedding)\n",
        "            else:\n",
        "                W = torch.Tensor(load_embeddings(args.embed_file))\n",
        "\n",
        "            self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
        "            self.embed.weight.data = W.clone()\n",
        "        else:\n",
        "            # add 2 to include UNK and PAD\n",
        "            self.embed = nn.Embedding(len(dicts['w2ind']) + 2, args.embed_size, padding_idx=0)\n",
        "        self.feature_size = self.embed.embedding_dim\n",
        "\n",
        "        self.embed_drop = nn.Dropout(p=args.dropout)\n",
        "\n",
        "        self.conv_dict = {1: [self.feature_size, args.num_filter_maps],\n",
        "                     2: [self.feature_size, 100, args.num_filter_maps],\n",
        "                     3: [self.feature_size, 150, 100, args.num_filter_maps],\n",
        "                     4: [self.feature_size, 200, 150, 100, args.num_filter_maps]\n",
        "                     }\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [self.embed(x)]\n",
        "\n",
        "        x = torch.cat(features, dim=2)\n",
        "\n",
        "        x = self.embed_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zfk5D7hvGolD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ResidualBlock module provides a building block for a deeper 1D CNN architecture with residual connections, which can be used for a variety of sequence modeling tasks"
      ],
      "metadata": {
        "id": "w9qoQr8iDuiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, kernel_size, stride, use_res, dropout):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv1d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=int(floor(kernel_size / 2)), bias=False),\n",
        "            nn.BatchNorm1d(outchannel),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(outchannel, outchannel, kernel_size=kernel_size, stride=1, padding=int(floor(kernel_size / 2)), bias=False),\n",
        "            nn.BatchNorm1d(outchannel)\n",
        "        )\n",
        "\n",
        "        self.use_res = use_res\n",
        "        if self.use_res:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                        nn.Conv1d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                        nn.BatchNorm1d(outchannel)\n",
        "                    )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.left(x)\n",
        "        if self.use_res:\n",
        "            out += self.shortcut(x)\n",
        "        out = torch.tanh(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SIaiUojDG6RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomlyInitializedDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The original per-label attention network: query matrix is randomly initialized\n",
        "    \"\"\"\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(RandomlyInitializedDecoder, self).__init__()\n",
        "\n",
        "        Y = Y[-1]\n",
        "\n",
        "        self.U = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.U.weight)\n",
        "\n",
        "\n",
        "        self.final = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.final.weight)\n",
        "\n",
        "        self.loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        # attention\n",
        "        alpha = F.softmax(self.U.weight.matmul(x.transpose(1, 2)), dim=2)\n",
        "\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "        return y, loss, alpha, m\n",
        "    \n",
        "    def change_depth(self, depth=0):\n",
        "        # placeholder\n",
        "        pass"
      ],
      "metadata": {
        "id": "Jjip3zUAHLxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Decoder module computes a binary cross-entropy loss or an asymmetric loss between \n",
        "# the predicted logits and the target labels, depending on the specified loss function, \n",
        "# and returns the predicted logits, loss, attention weights, and context vector as output.\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder: knowledge transfer initialization and hyperbolic embedding correction\n",
        "    \"\"\"\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dicts = dicts\n",
        "\n",
        "        self.decoder_dict = nn.ModuleDict()\n",
        "        for i in range(len(Y)):\n",
        "            y = Y[i]\n",
        "            self.decoder_dict[str(i) + '_' + '0'] = nn.Linear(input_size, y)\n",
        "            self.decoder_dict[str(i) + '_' + '1'] = nn.Linear(input_size, y)\n",
        "            xavier_uniform(self.decoder_dict[str(i) + '_' + '0'].weight)\n",
        "            xavier_uniform(self.decoder_dict[str(i) + '_' + '1'].weight)\n",
        "        \n",
        "        self.use_hyperbolic =  args.decoder.find(\"Hyperbolic\") != -1\n",
        "        if self.use_hyperbolic:\n",
        "            self.cat_hyperbolic = args.cat_hyperbolic\n",
        "            if not self.cat_hyperbolic:\n",
        "                self.hyperbolic_fc_dict = nn.ModuleDict()\n",
        "                for i in range(len(Y)):\n",
        "                    self.hyperbolic_fc_dict[str(i)] = nn.Linear(args.hyperbolic_dim, input_size)\n",
        "            else:\n",
        "                self.query_fc_dict = nn.ModuleDict()\n",
        "                for i in range(len(Y)):\n",
        "                    self.query_fc_dict[str(i)] = nn.Linear(input_size + args.hyperbolic_dim, input_size)\n",
        "            \n",
        "            # build hyperbolic embedding matrix\n",
        "            self.hyperbolic_emb_dict = {}\n",
        "            for i in range(len(Y)):\n",
        "                self.hyperbolic_emb_dict[i] = np.zeros((Y[i], args.hyperbolic_dim))\n",
        "                for idx, code in dicts['ind2c'][i].items():\n",
        "                    self.hyperbolic_emb_dict[i][idx, :] = np.copy(dicts['poincare_embeddings'].get_vector(code))\n",
        "                self.register_buffer(name='hb_emb_' + str(i), tensor=torch.tensor(self.hyperbolic_emb_dict[i], dtype=torch.float32))\n",
        "\n",
        "        self.cur_depth = 5 - args.depth\n",
        "        self.is_init = False\n",
        "        self.change_depth(self.cur_depth)\n",
        "\n",
        "        if args.loss == 'BCE':\n",
        "            self.loss_function = nn.BCEWithLogitsLoss()\n",
        "        elif args.loss == 'ASL':\n",
        "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
        "            self.loss_function = AsymmetricLoss(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
        "                                                clip=asl_config[2], reduction=args.asl_reduction)\n",
        "        elif args.loss == 'ASLO':\n",
        "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
        "            self.loss_function = AsymmetricLossOptimized(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
        "                                                         clip=asl_config[2], reduction=args.asl_reduction)\n",
        "    \n",
        "    def change_depth(self, depth=0):\n",
        "        if self.is_init:\n",
        "            # copy previous attention weights to current attention network based on ICD hierarchy\n",
        "            ind2c = self.dicts['ind2c']\n",
        "            c2ind = self.dicts['c2ind']\n",
        "            hierarchy_dist = self.dicts['hierarchy_dist']\n",
        "            for i, code in ind2c[depth].items():\n",
        "                tree = hierarchy_dist[depth][code]\n",
        "                pre_idx = c2ind[depth - 1][tree[depth - 1]]\n",
        "\n",
        "                self.decoder_dict[str(depth) + '_' + '0'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '0'].weight.data[pre_idx, :].clone()\n",
        "                self.decoder_dict[str(depth) + '_' + '1'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '1'].weight.data[pre_idx, :].clone()\n",
        "\n",
        "        if not self.is_init:\n",
        "            self.is_init = True\n",
        "\n",
        "        self.cur_depth = depth\n",
        "        \n",
        "    def forward(self, x, target, text_inputs):\n",
        "        # attention\n",
        "        if self.use_hyperbolic:\n",
        "            if not self.cat_hyperbolic:\n",
        "                query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight + self.hyperbolic_fc_dict[str(self.cur_depth)](self._buffers['hb_emb_' + str(self.cur_depth)])\n",
        "            else:\n",
        "                query = torch.cat([self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight, self._buffers['hb_emb_' + str(self.cur_depth)]], dim=1)\n",
        "                query = self.query_fc_dict[str(self.cur_depth)](query)\n",
        "        else:\n",
        "            query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight\n",
        "\n",
        "        alpha = F.softmax(query.matmul(x.transpose(1, 2)), dim=2)\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.decoder_dict[str(self.cur_depth) + '_' + '1'].weight.mul(m).sum(dim=2).add(self.decoder_dict[str(self.cur_depth) + '_' + '1'].bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "        \n",
        "        return y, loss, alpha, m"
      ],
      "metadata": {
        "id": "BEwexuIHHW8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the MultiResCNN class provides a flexible and powerful architecture for multi-label text \n",
        "# classification tasks, with the ability to incorporate pre-trained embeddings, \n",
        "# hierarchical label structures, and hyperbolic embeddings.\n",
        "class MultiResCNN(nn.Module):\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(MultiResCNN, self).__init__()\n",
        "\n",
        "        self.word_rep = WordRep(args, Y, dicts)\n",
        "\n",
        "        self.conv = nn.ModuleList()\n",
        "        filter_sizes = args.filter_size.split(',')\n",
        "\n",
        "        self.filter_num = len(filter_sizes)\n",
        "        for filter_size in filter_sizes:\n",
        "            filter_size = int(filter_size)\n",
        "            one_channel = nn.ModuleList()\n",
        "            tmp = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
        "                            padding=int(floor(filter_size / 2)))\n",
        "            xavier_uniform(tmp.weight)\n",
        "            one_channel.add_module('baseconv', tmp)\n",
        "            one_channel.add_module('base_bn', nn.BatchNorm1d(self.word_rep.feature_size))\n",
        "\n",
        "            conv_dimension = self.word_rep.conv_dict[args.conv_layer]\n",
        "            for idx in range(args.conv_layer):\n",
        "                tmp = ResidualBlock(conv_dimension[idx], conv_dimension[idx + 1], filter_size, 1, True,\n",
        "                                    args.dropout)\n",
        "                one_channel.add_module('resconv-{}'.format(idx), tmp)\n",
        "\n",
        "            self.conv.add_module('channel-{}'.format(filter_size), one_channel)\n",
        "\n",
        "        if args.decoder == \"HierarchicalHyperbolic\" or args.decoder == \"Hierarchical\":\n",
        "            self.decoder = Decoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        elif args.decoder == \"RandomlyInitialized\":\n",
        "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        elif args.decoder == \"CodeTitle\":\n",
        "            self.decoder = RACDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        else:\n",
        "            raise RuntimeError(\"wrong decoder name\")\n",
        "\n",
        "        self.cur_depth = 5 - args.depth\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        x = self.word_rep(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        conv_result = []\n",
        "        for conv in self.conv:\n",
        "            tmp = x\n",
        "            for idx, md in enumerate(conv):\n",
        "                if idx % 2 == 0:\n",
        "                    tmp = F.relu(md(tmp))\n",
        "                else:\n",
        "                    tmp = md(tmp)\n",
        "            tmp = tmp.transpose(1, 2)\n",
        "            conv_result.append(tmp)\n",
        "        x = torch.cat(conv_result, dim=2)\n",
        "\n",
        "        y, loss, alpha, m = self.decoder(x, target, text_inputs)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def freeze_net(self):\n",
        "        for p in self.word_rep.embed.parameters():\n",
        "            p.requires_grad = False"
      ],
      "metadata": {
        "id": "oeiRK0v8v0-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --MODEL_DIR /content/drive/MyDrive/HiCu-ICD-main/models \\\n",
        "    --DATA_DIR /content/drive/MyDrive/HiCu-ICD-main/data \\\n",
        "    --MIMIC_3_DIR /content/drive/MyDrive/HiCu-ICD-main/data/mimic3 \\\n",
        "    --data_path /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/train_50.csv \\\n",
        "    --embed_file /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/processed_full_100.embed \\\n",
        "    --vocab /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/vocab.csv \\\n",
        "    --Y full \\\n",
        "    --model MultiResCNN \\\n",
        "    --decoder RandomlyInitialized \\\n",
        "    --criterion prec_at_8 \\\n",
        "    --MAX_LENGTH 1024 \\\n",
        "    --batch_size 2  \\\n",
        "    --lr 5e-5 \\\n",
        "    --depth 5 \\\n",
        "    --n_epochs '2,3,3,3,5'  \\\n",
        "    --num_workers 8 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aPGFQvKFX98",
        "outputId": "b02231bb-f7ff-4ff6-fc03-4a53097d8e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-07 18:13:56.919475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Namespace(MODEL_DIR='/content/drive/MyDrive/HiCu-ICD-main/models', DATA_DIR='/content/drive/MyDrive/HiCu-ICD-main/data', MIMIC_3_DIR='/content/drive/MyDrive/HiCu-ICD-main/data/mimic3', MIMIC_2_DIR='./data/mimic2', data_path='/content/drive/MyDrive/HiCu-ICD-main/data/mimic3/train_50.csv', vocab='/content/drive/MyDrive/HiCu-ICD-main/data/mimic3/vocab.csv', Y='full', version='mimic3', MAX_LENGTH=1024, model='MultiResCNN', decoder='RandomlyInitialized', filter_size='3,5,9,15,19,25', num_filter_maps=50, conv_layer=1, embed_file='/content/drive/MyDrive/HiCu-ICD-main/data/mimic3/processed_full_100.embed', hyperbolic_dim=50, test_model=None, use_ext_emb=False, cat_hyperbolic=False, loss='BCE', asl_config='0,0,0', asl_reduction='sum', n_epochs='2,3,3,3,5', depth=5, dropout=0.2, patience=10, batch_size=2, lr=5e-05, weight_decay=0, criterion='prec_at_8', gpu='0', num_workers=8, tune_wordemb=True, random_seed=1, thres=0.5, longformer_dir='', reader_conv_num=2, reader_trans_num=4, trans_ff_dim=1024, num_code_title_tokens=36, code_title_filter_size=9, lstm_hidden_dim=512, attn_dim=512, scheduler=0.9, scheduler_patience=5, command='python main.py --MODEL_DIR /content/drive/MyDrive/HiCu-ICD-main/models --DATA_DIR /content/drive/MyDrive/HiCu-ICD-main/data --MIMIC_3_DIR /content/drive/MyDrive/HiCu-ICD-main/data/mimic3 --data_path /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/train_50.csv --embed_file /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/processed_full_100.embed --vocab /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/vocab.csv --Y full --model MultiResCNN --decoder RandomlyInitialized --criterion prec_at_8 --MAX_LENGTH 1024 --batch_size 2 --lr 5e-5 --depth 5 --n_epochs 2,3,3,3,5 --num_workers 8', gpu_list=[0])\n",
            "loading lookups...\n",
            "Depth 0: 14\n",
            "Depth 1: 30\n",
            "Depth 2: 39\n",
            "Depth 3: 47\n",
            "Depth 4: 50\n",
            "loading pretrained embeddings from /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/processed_full_100.embed\n",
            "adding unk embedding\n",
            "MultiResCNN(\n",
            "  (word_rep): WordRep(\n",
            "    (embed): Embedding(51921, 100, padding_idx=0)\n",
            "    (embed_drop): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (conv): ModuleList(\n",
            "    (0): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (2): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (3): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (4): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(19,), stride=(1,), padding=(9,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (5): ModuleList(\n",
            "      (0): Conv1d(100, 100, kernel_size=(25,), stride=(1,), padding=(12,))\n",
            "      (1): ResidualBlock(\n",
            "        (left): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): Tanh()\n",
            "          (3): Conv1d(50, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
            "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (shortcut): Sequential(\n",
            "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
            "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): RandomlyInitializedDecoder(\n",
            "    (U): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (final): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (loss_function): BCEWithLogitsLoss()\n",
            "  )\n",
            ")\n",
            "train_instances 8066\n",
            "dev_instances 1573\n",
            "test_instances 1729\n",
            "Total epochs at each level: [2, 3, 3, 3, 5]\n",
            "Training model at depth 4:\n",
            "EPOCH 0\n",
            "/content/drive/.shortcut-targets-by-id/1i33jnEExQNdrh2x4f7zRXTC8io_INa2u/HiCu-ICD-main/utils/train_test.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  inputs_id, labels = torch.LongTensor(inputs_id), torch.FloatTensor(labels[cur_depth])\n",
            "epoch finish in 80.96s, loss: 0.3089\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.1237, 0.2984, 0.1431, 0.1934, 0.7352\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.1881, 0.7085, 0.2039, 0.3167, 0.7956\n",
            "rec_at_5: 0.3930\n",
            "prec_at_5: 0.4281\n",
            "rec_at_8: 0.4997\n",
            "prec_at_8: 0.3474\n",
            "rec_at_15: 0.6672\n",
            "prec_at_15: 0.2528\n",
            "\n",
            "evaluation finish in 14.48s\n",
            "saved metrics, params, model to directory /content/drive/MyDrive/HiCu-ICD-main/models/MultiResCNN_RandomlyInitialized_May_07_18_14_26\n",
            "\n",
            "EPOCH 1\n",
            "epoch finish in 62.66s, loss: 0.2637\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.1809, 0.4195, 0.2140, 0.2834, 0.7804\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.2519, 0.6909, 0.2839, 0.4024, 0.8350\n",
            "rec_at_5: 0.4461\n",
            "prec_at_5: 0.4734\n",
            "rec_at_8: 0.5563\n",
            "prec_at_8: 0.3792\n",
            "rec_at_15: 0.7352\n",
            "prec_at_15: 0.2751\n",
            "\n",
            "evaluation finish in 8.38s\n",
            "saved metrics, params, model to directory /content/drive/MyDrive/HiCu-ICD-main/models/MultiResCNN_RandomlyInitialized_May_07_18_14_26\n",
            "\n",
            "EPOCH 2\n",
            "epoch finish in 58.92s, loss: 0.2447\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.2202, 0.4692, 0.2592, 0.3339, 0.8061\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.2731, 0.7009, 0.3092, 0.4291, 0.8522\n",
            "rec_at_5: 0.4712\n",
            "prec_at_5: 0.4968\n",
            "rec_at_8: 0.5896\n",
            "prec_at_8: 0.4004\n",
            "rec_at_15: 0.7635\n",
            "prec_at_15: 0.2866\n",
            "\n",
            "evaluation finish in 8.43s\n",
            "saved metrics, params, model to directory /content/drive/MyDrive/HiCu-ICD-main/models/MultiResCNN_RandomlyInitialized_May_07_18_14_26\n",
            "\n",
            "EPOCH 3\n",
            "epoch finish in 56.27s, loss: 0.2300\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.2787, 0.5371, 0.3378, 0.4147, 0.8259\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.3414, 0.6786, 0.4073, 0.5091, 0.8690\n",
            "rec_at_5: 0.5155\n",
            "prec_at_5: 0.5358\n",
            "rec_at_8: 0.6382\n",
            "prec_at_8: 0.4318\n",
            "rec_at_15: 0.7893\n",
            "prec_at_15: 0.2957\n",
            "\n",
            "evaluation finish in 8.61s\n",
            "saved metrics, params, model to directory /content/drive/MyDrive/HiCu-ICD-main/models/MultiResCNN_RandomlyInitialized_May_07_18_14_26\n",
            "\n",
            "EPOCH 4\n",
            "epoch finish in 56.16s, loss: 0.2170\n",
            "last epoch: testing on dev and test sets\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/dev_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.3115, 0.5425, 0.3799, 0.4469, 0.8377\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.3606, 0.7022, 0.4257, 0.5301, 0.8782\n",
            "rec_at_5: 0.5302\n",
            "prec_at_5: 0.5470\n",
            "rec_at_8: 0.6558\n",
            "prec_at_8: 0.4414\n",
            "rec_at_15: 0.8093\n",
            "prec_at_15: 0.3030\n",
            "\n",
            "evaluation finish in 8.59s\n",
            "file for evaluation: /content/drive/MyDrive/HiCu-ICD-main/data/mimic3/test_50.csv\n",
            "\n",
            "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.3067, 0.5280, 0.3766, 0.4397, 0.8372\n",
            "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
            "0.3554, 0.6948, 0.4211, 0.5244, 0.8791\n",
            "rec_at_5: 0.5208\n",
            "prec_at_5: 0.5499\n",
            "rec_at_8: 0.6436\n",
            "prec_at_8: 0.4466\n",
            "rec_at_15: 0.8096\n",
            "prec_at_15: 0.3113\n",
            "\n",
            "saved metrics, params, model to directory /content/drive/MyDrive/HiCu-ICD-main/models/MultiResCNN_RandomlyInitialized_May_07_18_14_26\n",
            "\n"
          ]
        }
      ]
    }
  ]
}